## transformer的分词器

在数据集上训练模型之前，需要将其预处理为预期的模型输入格式。无论您的数据是文本、图像还是音频，都需要将其转换并组装成一匹匹的张量。
Transformers 提供了一组预处理类，以帮助为模型准备数据。

- Text，使用 Tokenizer 将文本转换为标记序列，创建标记的数字表示，并将它们组装成张量。
- 语音和音频，使用特征提取器从音频波形中提取顺序特征并将其转换为张量。
- 图像输入使用 ImageProcessor 将图像转换为张量。
- 多模态输入，使用 Processor 将分词器和特征提取器或图像处理器组合在一起。


####  文本

预处理文本数据的主要工具是分词器。
分词器根据一组规则将文本拆分为分词。标记被转换为数字，然后是张量，张量成为模型输入。模型所需的任何其他输入都由 tokenizer 添加。
首先使用 AutoTokenizer.from_pretrained（） 方法加载预训练的分词器。这将下载模型预训练时使用的词汇表，然后将您的文本传递给分词器。

分词器返回一个包含三个重要项的字典：

- input_ids 是句子中每个token对应的索引。
- attention_mask 指示是否应关注token。
- token_type_ids 标识当存在多个序列时，token属于哪个序列

分词器在句子中添加了两个特殊标记 - CLS 和 SEP （分类器和分隔符）。并非所有模型都需要特殊令牌，但如果它们需要，分词器会自动为您添加它们。

如果要预处理多个句子，请将它们作为列表传递给分词器。

对文本进行分词是将其拆分为单词或子词，然后通过查找表将其转换为 id。将单词或子词转换为 id 很简单，但是将文本拆分为单词或子词（即对文本进行标记化）有不同的方法。
Transformer 中使用的三种主要类型的分词器：
- 字节对编码 （BPE）、
- WordPiece
- SentencePiece
并展示哪个模型使用哪种分词器类型的示例。

请注意，在每个模型页面上，您可以查看相关 tokenizer 的文档，以了解预训练模型使用了哪种 tokenizer 类型。例如，如果我们查看 BertTokenizer，我们可以看到该模型使用了 WordPiece。


##### 文本拆分成介绍

将文本拆分成更小的块是一项比看起来更难的任务，并且有多种方法可以做到这一点。例如，让我们看看句子 "Don't you love 🤗 Transformers? We sure do."
标记此文本的一种简单方法是将其按空格拆分，这将得到：["Don't", "you", "love", "🤗", "Transformers?", "We", "sure", "do."]

这是明智的第一步，但如果我们查看标记 “Transformers？” 和 “do.”，我们会注意到标点符号附加到单词 “Transformer” 和 “do” 上，这是次优的。我们应该考虑标点符号，这样模型就不必学习单词的不同表示形式以及可能跟随它的每个可能的标点符号，这将使模型必须学习的表示数量激增。考虑到标点符号，对我们的示例文本进行标记将得到：
["Don", "'", "t", "you", "love", "🤗", "Transformers", "?", "We", "sure", "do", "."]

更好。然而，标记化如何处理 “Don't” 这个词是不利的。“Don't” 代表 “do not”，因此最好将其标记为 [“Do”， “n't”]。这就是事情开始变得复杂的地方，也是每个模型都有自己的 tokenizer 类型的部分原因。根据我们应用于文本分词的规则，会为同一文本生成不同的分词化输出。只有当您向预训练模型提供一个输入时，该输入使用用于标记其训练数据的相同规则进行标记化，才能正确执行。

spaCy 和 Moses 是两种流行的基于规则的分词器。在的示例中应用它们，spaCy 和 Moses 将输出如下内容：
["Do", "n't", "you", "love", "🤗", "Transformers", "?", "We", "sure", "do", "."]

可以看出，这里使用了空格和标点符号分词，以及基于规则的分词化。空格和标点符号分词以及基于规则的分词都是单词分词化的示例，后者被粗略地定义为将句子拆分为单词。虽然这是将文本拆分为较小块的最直观方法，但这种标记化方法可能会导致大量文本语料库出现问题。在这种情况下，空格和标点符号标记化通常会生成一个非常大的词汇表（使用的所有唯一单词和标记的集合）。例如，Transformer XL 使用空格和标点符号分词，导致词汇大小为 267,735！

如此大的词汇量迫使模型具有一个巨大的嵌入矩阵作为输入和输出层，这会导致内存和时间复杂性增加。一般来说，transformers 模型的词汇量很少大于 50,000，尤其是在它们仅针对单一语言进行预训练时。

那么，如果简单的空格和标点符号分词不令人满意，为什么不简单地对字符进行分词呢？

虽然字符标记化非常简单，并且可以大大降低内存和时间复杂性，但会使模型更难学习有意义的输入表示。例如，为字母 “t” 学习有意义的上下文无关的表示比学习单词的上下文无关的表示要困难得多 “今天”。因此，字符标记化通常伴随着性能损失。因此，为了两全其美，Transformers 模型使用了单词级和字符级标记化的混合体，称为 subword 标记化。

##### Subword tokenization 子词分词
子词分词算法基于以下原则：经常使用的词不应拆分为较小的子词，而应将稀有词分解为有意义的子词。例如，“annoyingly”可能被认为是一个稀有词，可以分解为“annoying”和“ly”。“annoying” 和 “ly” 作为独立的子词会出现得更频繁，而同时 “annoyingly” 的含义则由 “annoying” 和 “ly” 的复合含义保留。这在土耳其语等凝集语言中特别有用，在土耳其语中，您可以通过将子词串在一起来形成（几乎）任意长的复杂单词。

子词分词化允许模型具有合理的词汇量，同时能够学习有意义的、独立于上下文的表示。此外，子词标记化使模型能够通过将单词分解为已知的子词来处理它以前从未见过的单词。例如，BertTokenizer 对
I have a new GPU!

结果：["i", "have", "a", "new", "gp", "##u", "!"]

因为我们考虑的是 uncased 模型，所以句子首先是小写的。我们可以看到单词 [“i”， “have”， “a”， “new”] 出现在分词器的词汇表中，但单词 “gpu” 不存在。因此，分词器将 “gpu” 拆分为已知的子词：[“gp” 和 “##u”]。“##” 表示令牌的其余部分应附加到前一个令牌上，没有空格（用于解码或反转令牌化）

现在让我们看看不同的子词分词化算法是如何工作的。请注意，所有这些标记化算法都依赖于某种形式的训练，这通常是在将要训练相应模型的语料库上完成的。

##### Byte-Pair Encoding (BPE) 字节对编码 （BPE）
字节对编码 （BPE） 是在 Neural Machine Translation of Rare Words with Subword Units （Sennrich et al.， 2015） 中引入的。BPE 依赖于将训练数据拆分为单词的预分词器。预标记化可以像空间标记化一样简单，例如 GPT-2、RoBERTa。更高级的预标记化包括基于规则的标记化，例如 XLM、 FlauBERT 对大多数语言使用 Moses，或者 GPT 使用 spaCy 和 ftfy 来计算训练语料库中每个单词的频率。

在预分词化之后，创建了一组唯一的单词，并确定了每个单词在训练数据中出现的频率。接下来，BPE 创建一个由唯一单词集中出现的所有符号组成的基本词汇表，并学习合并规则以从基本词汇表的两个符号中形成一个新符号。它会一直这样做，直到词汇表达到所需的词汇表大小。请注意，所需的词汇表大小是在训练分词器之前要定义的超参数。

BPE 会计算每个可能的符号对的频率，并选择出现频率最高的符号对。
假设 Byte-Pair Encoding 训练此时停止，则学习的合并规则将应用于新单词（只要这些新单词不包含不在基本词汇表中的符号）。例如，单词 “bug” 将被标记为 [“b”， “ug”] ，但 “mug” 将被标记为 [“<unk>”， “ug”] ，因为符号 “m” 不在基本词汇表中。通常，“m”等单个字母不会替换为 “<unk>” 符号，因为训练数据通常包含每个字母至少出现一次，但对于表情符号等非常特殊的字符，则可能会发生这种情况。

如前所述，词汇表大小（即基本词汇表大小 + 合并次数）是一个可供选择的超参数。例如，GPT 的词汇量为 40,478，因为它们***有 478 个基本字符***，并选择在 ***40,000 次合并***后停止训练。

##### Byte-level BPE 字节级 BPE
如果将所有 unicode 字符都视为基本字符，则包含所有可能的基本字符的基本词汇表可能非常大。为了获得更好的基本词汇表，GPT-2 使用字节作为基本词汇表，这是一个聪明的技巧，可以强制基本词汇表的大小为 256，同时确保每个基本字符都包含在词汇表中。通过一些额外的规则来处理标点符号，GPT2 的分词器可以在不需要符号的情况下对每个文本进行分词。GPT-2 的词汇量为 50,257，对应于***256字节*** 的基本标记、特殊的文本结束标记和通过 ***50,000 次合并学习***的符号。

##### WordPiece WordPiece （英文）
WordPiece 是用于 BERT、DistilBERT 和 Electra 的子词标记化算法。该算法在日语和韩语语音搜索中进行了概述（Schuster et al.， 2012），与 BPE 非常相似。WordPiece 首先初始化词汇表以包含训练数据中存在的每个字符，然后逐步学习给定数量的合并规则。与 BPE 相比，WordPiece 不选择最频繁的符号对，而是选择一旦添加到词汇表中，训练数据的可能性最大化的符号对。

那么这到底意味着什么呢？参考前面的示例，最大化训练数据的可能性等效于找到符号对，其概率除以第一个符号后跟第二个符号的概率是所有符号对中最大的。例如，如果 “ug” 除以 “u”， “g” 的概率大于任何其他符号对的概率，则 “u” 后跟 “g” 才会被合并。直观地说，WordPiece 与 BPE 略有不同，因为它通过合并两个符号来评估它所失去的东西，以确保它是值得的。

##### Unigram 统一拼图
Unigram 是 Subword Regularization： Improving Neural Network Translation Models with Multiple Subword Candidate （Kudo， 2018） 中介绍的一种子词分词化算法。与 BPE 或 WordPiece 相比，Unigram 将其基本词汇表初始化为大量符号，并逐渐缩减每个符号以获得更小的词汇表。例如，基本词汇表可以对应于所有预标记化的单词和最常见的子字符串。Unigram 不直接用于 transformer 中的任何模型，但它与 SentencePiece 结合使用。

在每个训练步骤中，Unigram 算法在给定当前词汇和 unigram 语言模型的情况下定义对训练数据的损失（通常定义为对数似然）。然后，对于词汇表中的每个符号，该算法会计算如果从词汇表中删除该符号，总体损失会增加多少。然后，Unigram 删除损失增加最低的符号的 p（p 通常为 10% 或 20%）百分比，即那些对训练数据的整体损失影响最小的符号。重复此过程，直到词汇表达到所需的大小。Unigram 算法始终保留基本字符，以便任何单词都可以进行标记化。

由于 Unigram 不基于合并规则（与 BPE 和 WordPiece 相比），因此该算法在训练后有几种方法可以对新文本进行标记。

##### SentencePiece SentencePiece 语句
到目前为止描述的所有分词化算法都存在相同的问题：假设输入文本使用空格来分隔单词。
但是，并非所有语言都使用空格来分隔单词。一种可能的解决方案是使用特定于语言的预分词器，
例如 XLM 使用特定的中文、日语和泰语预分词器。
为了更普遍地解决这个问题，
SentencePiece：一种用于神经文本处理的简单且独立于语言的子词分词器和解词器（Kudo et al.， 2018）将输入视为原始输入流，从而在要使用的字符集中包含空格。
然后，它使用 BPE 或 unigram 算法来构建相应的词汇表。

XLNetTokenizer 以 SentencePiece 为例，这也是为什么在前面的示例中， “ ” 字符包含在词汇表中。使用 SentencePiece 进行解码非常简单，因为所有标记都可以连接，并且 “ ” 被空格替换。
库中所有使用 SentencePiece 的 transformer 模型都将其与 unigram 结合使用。使用 SentencePiece 的模型示例包括 ALBERT、XLNet、Marian 和 T5。


#### Pad

句子的长度并不总是相同的，这可能是一个问题，因为模型输入的张量需要具有统一的形状。Padding 是一种通过向较短的句子添加特殊 Padding 标记来确保张量为矩形的策略。
将 padding 参数设置为 True 以填充批处理中较短的序列以匹配最长的序列

#### Truncation 截断

另一方面，有时序列可能太长，模型无法处理。在这种情况下，您需要将序列截断为更短的长度。
将 truncation 参数设置为 True 可将序列截断为模型接受的最大长度。

最后，您希望 tokenizer 返回馈送到模型的实际张量。将 return_tensors 参数设置为 pt（对于 PyTorch）或 tf（对于 TensorFlow）。

### Audio 音频

对于音频任务，您需要一个特征提取器来为模型准备数据集。特征提取器旨在从原始音频数据中提取特征，并将其转换为张量。
如何将特征提取器与音频数据集一起使用：
访问 audio 列的第一个元素以查看输入。调用 audio 列会自动加载和重新采样音频文件：
这将返回 3 项：

- array 是作为 1D 数组加载并可能重新采样的语音信号
- path 指向音频文件的位置
- sampling_rate是指每秒测量语音信号中的数据点数

音频数据的采样率必须与用于预训练模型的数据集的采样率相匹配。如果数据的采样率不同，则需要对数据进行重新采样

接下来，加载特征提取器以规范化和填充输入。填充文本数据时，会为较短的序列添加 0。同样的想法也适用于音频数据。特征提取器将 0 - 解释为 silence - 添加到数组中。

将 audio 数组传递给特征提取器。我们还建议在功能提取器中添加 sampling_rate 参数，以便更好地调试可能发生的任何静默错误。

就像 tokenizer 一样，您可以应用填充或截断来处理批量中的变量序列。

### Computer vision 计算机视觉

对于计算机视觉任务，需要一个图像处理器来为模型准备数据集。
图像预处理包括几个步骤，这些步骤将图像转换为模型所需的输入。
这些步骤包括但不限于调整大小、标准化、颜色通道校正以及将图像转换为张量。

图像预处理通常遵循某种形式的图像增强。图像预处理和图像增强都会转换图像数据，但它们的用途不同：

- 图像增强以有助于防止过度拟合并提高模型稳健性的方式改变图像。可以在如何增强数据方面发挥创意 - 调整亮度和颜色、裁剪、旋转、调整大小、缩放等。但是，请注意不要通过增强来改变图像的含义。
- 图像预处理可保证图像与模型的预期输入格式匹配。在微调计算机视觉模型时，必须完全按照最初训练模型时的方式对图像进行预处理。

可以使用任何喜欢的库进行图像增强。对于图像预处理，请使用与模型关联的 ImageProcessor。


首先，添加一些图像增广。可以使用您喜欢的任何库，比如 torchvision 的 transforms 模块。
如果有兴趣使用其他数据增强库，请在 Albumentations 或 Kornia 笔记本中了解如何操作

- 使用 Compose 将几个转换 - RandomResizedCrop 和 ColorJitter 链接在一起。请注意，对于调整大小，可以从 image_processor 获取图像大小要求。对于某些模型，需要精确的高度和宽度，而对于其他模型，只需定义shortest_edge
- 模型接受 pixel_values 作为其输入。ImageProcessor 可以负责对图像进行标准化，并生成适当的张量。创建一个函数，将图像增广和图像预处理相结合，为一批图像生成pixel_values

***设置了 do_resize=False，在图像增强转换中调整了图像的大小，并利用了相应image_processor的 size 属性。如果在图像增强期间不调整图像大小，请省略此参数。默认情况下，ImageProcessor 将处理大小调整。***
***如果希望将图像标准化作为增强转换的一部分，请使用 image_processor.image_mean 和 image_processor.image_std 值***

#### Pad 垫
在某些情况下，例如，在微调 DETR 时，模型会在训练时应用缩放增强。这可能会导致图像在批次中大小不同。您可以使用 DetrImageProcessor.pad（） 并定义一个自定义collate_fn以一起批处理图像。


### Multimodal 模 态

对于涉及多模态输入的任务，需要一个处理器来为模型准备数据集。处理器将两个处理对象（如 tokenizer 和 feature extractor）耦合在一起。

对于 ASR，主要关注音频和文本，因此可以删除其他列。

对音频数据集的采样率进行重新采样，以匹配用于预训练模型的数据集的采样率！


