## Orca：基于 Transformer 的生成模型的分布式服务系统

为生成任务训练的基于 Transformer 的大规模模型（例如 GPT-3）最近引起了极大的兴趣，强调了为该系列中的模型提供服务的系统支持的需求。由于这些模型以自回归方式生成下一个标记，因此必须多次运行模型才能处理推理请求，其中模型的每次迭代都会为请求生成一个输出标记。然而，现有的推理服务系统在这种具有多次迭代特性的工作负载上表现不佳，因为它们的调度机制不灵活，无法改变当前正在处理的请求批次;比批处理中的其他请求更早完成的请求无法返回给客户端，而新到达的请求必须等到当前批处理完全完成。

Orca提出了迭代级调度，这是一种新的调度机制，它以迭代的粒度（而不是请求）调度执行，其中调度器调用执行引擎以仅在批处理上运行模型的单个迭代。此外，要同时将批处理和迭代级调度应用于 Transformer 模型，建议选择性批处理，即仅将批处理应用于一组选定的操作。基于这两种技术，实现了一个名为 ORCA 的分布式服务系统，并进行了额外的设计，以扩展到具有数千亿个参数的模型。对 GPT-3 175B 模型的评估表明， ORCA 在延迟和吞吐量方面都明显优于 NVIDIA FasterTransformer：在相同的延迟水平下，吞吐量提高了 36：9× 。

![Orca-paper](https://www.usenix.org/system/files/osdi22-yu.pdf)