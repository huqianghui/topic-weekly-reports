## TensorRT-LLM

大语言模型规模庞大。如果不采用正确的技术，那么运行大语言模型的成本不仅会很高，速度也会很慢。
为解决这一问题，从模型优化（如内核融合和量化）到运行时优化（如 C++ 实现、KV 缓存、连续动态批处理 continuous in-flight batching 和分页注意力 paged attention），众多优化技术应运而生。但很难确定哪种技术适合您的用例，也很难在这些实施中有时并不兼容的技术间进行交互。

因此，NVIDIA 推出了 TensorRT-LLM，它是一个专门用于编译和优化大语言模型推理的综合程序库。TensorRT-LLM 整合了所有这些优化功能，同时还提供了一个直观的 Python API 来定义和构建新模型。

TensorRT-LLM 开源程序库可加快 NVIDIA GPU 上最新大语言模型的推理性能。它是 NVIDIA NeMo 中优化大语言模型推理的骨干力量。NeMo 是一个用于构建和定制生成式 AI 应用并将其部署到生产中的端到端框架，为生成式 AI 的部署提供了完整的容器，如 TensorRT-LLM 和 NVIDIA Triton 等。

TensorRT-LLM 包含 TensorRT 的深度学习编译器，并采用了最新的优化内核，这些内核专为在前沿研究中实施用于 大语言模型 执行的 FlashAttention 和带掩码的多头注意力 (masked multi-head attention) 而打造。

TensorRT-LLM 还将预处理和后处理步骤以及多 GPU /多节点通信基元整合到一个简单的开源 Python API 中，可在 GPU 上实现突破性的大语言模型推理性能。

TensorRT-LLM 的特性如下：

1. 支持 Llama 1 和 2、Bloom、ChatGLM、Falcon、MPT、Baichuan 及 Starcoder 等 大语言模型
2. 动态批处理和分页注意力
3. 多 GPU 多节点（MGMN）推理
4. FP8 精度的 NVIDIA Hopper Transformer 引擎
5. 支持 NVIDIA Ampere 架构、NVIDIA Ada Lovelace 架构和 NVIDIA Hopper GPU
6. 原生 Windows 支持（测试版）

#### 检索模型权重

TensorRT-LLM 是一个用于大语言模型推理的程序库，因此要使用它，就需要提供一组训练过的权重。您可以使用自己在 NVIDIA NeMo 等框架中训练的模型权重，也可以从  HuggingFace Hub 等资源库中提取一组预训练权重。

本文中的命令会自动从 HuggingFace Hub 中提取 70 亿参数的 Llama 2 模型聊天调优变体的权重和分词器文件。您还可以使用以下命令，自行下载权重以供离线使用。您只需更新后续命令中的路径，使其指向此目录即可：

```cmd
git lfs install
git clone https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
```
该模型的使用受特定许可的限制。

#### 编译模型

下一步是将模型编译到 TensorRT 引擎中。为此，像定义模型权重那样，您也需要在 TensorRT-LLM Python API 中编写模型定义。

TensorRT-LLM 资源库包含多种预定义模型架构。在本文中，您可以使用其中包含的 Llama 模型定义，而无需自行编写。下面是 TensorRT-LLM 中一些可用优化的最简示例。

有关可用插件和量化的更多信息，参见完整的 Llama 示例和数值精度。

```cmd

# Launch the Tensorrt-LLM container
make -C docker release_run LOCAL_USER=1
 
# Log in to huggingface-cli
# You can get your token from huggingface.co/settings/token
huggingface-cli login --token *****
 
# Compile model
python3 examples/llama/build.py \
    --model_dir meta-llama/Llama-2-7b-chat-hf \
    --dtype float16 \
    --use_gpt_attention_plugin float16 \
    --use_gemm_plugin float16 \
    --remove_input_padding \
    --use_inflight_batching \
    --paged_kv_cache \
    --output_dir examples/llama/out
```

使用 TensorRT-LLM API 创建模型定义时，可以使用构成神经网络层的 NVIDIA TensorRT 基元来构建操作图。这些操作会映射到特定的内核，即为 GPU 预写的程序。

TensorRT 编译器可以对图进行扫描，为每个操作和可用的 GPU 选择最佳内核。最重要的是，它还能在图中识别出可以将多个操作“融合”到一个内核中的模式。这就减少了启动多个 GPU 内核所需的显存移动量和开销。

TensorRT 还能将操作图编译成单个 CUDA Graph，其可以一次性启动，从而进一步减少内核启动开销。

TensorRT 编译器在融合多个层和提高执行速度方面非常强大，但有些复杂的层融合（如 FlashAttention 等）会将许多操作交叉在一起，而且无法被自动发现。对此，可以在编译时使用插件来对图中的部分内容进行显式替换。

在本例中，您加入了 gpt_attention  插件（实现类似 FlashAttention 的融合注意力内核）和 gemm 插件（使用 FP32 累加执行矩阵乘法）。您还可以将完整模型的期望精度设为 FP16，以便与您从 HuggingFace 下载的权重的默认精度相匹配。

下面是该脚本运行后所生成的结果。现在，在 /examples/llama/out 文件夹中有以下文件：

1. Llama_float16_tp1_rank0.engine：构建脚本的主要输出，包含嵌入模型权重的可执行操作图。
2. config.json：包含模型的详细信息，如模型的一般结构和精度以及关于引擎中包含哪些插件的信息。
3. model.cache：缓存模型编译过程中的部分时序和优化信息，使后续构建更快。

#### 运行模型
现在您已经有了模型引擎，接下来该用它做什么呢？

引擎文件包含执行模型所需的信息，但在实际使用大语言模型时，需要的远不止是一次通过模型的前向传播。TensorRT-LLM 包含一个高度优化的 C++ 运行时，以执行已构建的大语言模型引擎和管理若干流程，如从模型输出结果中采样分词、管理 KV 缓存和批量处理请求等。

您可以直接使用该运行时在本地执行模型，也可以使用 NVIDIA Triton 推理服务器的 TensorRT-LLM 运行时后端为多个用户提供模型。

如要在本地运行模型，请执行以下命令：

```cmd
python3 examples/llama/run.py --engine_dir=examples/llama/out --max_output_len 100 --tokenizer_dir meta-llama/Llama-2-7b-chat-hf --input_text "How do I count to nine in French?"
```

