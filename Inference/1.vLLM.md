## vLLM

虚拟内存和分页的理念对于管理服务中的 LLM KV 缓存非常有效，因为工作负载(inference)需要动态内存分配（因为输出长度不是先验已知的），并且其性能受 GPU 内存容量的约束。
但是，这通常并不适用于所有 GPU 工作负载。例如，在 DNN (train)训练中，张量形状通常是静态的，因此可以提前优化内存分配。
再举一个例子，在提供不是 LLMs的 DNN 时，内存效率的提高可能不会带来任何性能改进，因为性能主要是计算绑定的。
在这种情况下，引入 vLLM 的技术可能会降低性能，因为内存间接和非连续块内存的额外开销。

LLMs 需要一次批处理足够多的请求。但是，现有系统很困难，因为每个请求的键值缓存 （KV cache） 内存很大，并且会动态增长和收缩。如果管理效率低下，这些内存可能会因碎片和冗余重复而严重浪费，从而限制批处理大小。为了解决这个问题，vLLM提出了 PagedAttention，这是一种受操作系统中经典虚拟内存和分页技术启发的注意力算法。
（1） KV 缓存内存的近乎零浪费
（2） 在请求内部和请求之间灵活共享 KV 缓存，以进一步减少内存使用。

与 FasterTransformer 和 Orca 等最先进的系统相比，vLLM 将 popular LLMs 的吞吐量提高了 2-4×而延迟水平相同。使用更长的序列、更大的模型和更复杂的解码算法，这种改进更加明显。
vLLM 的源代码在 https://github.com/vllm-project/vllm 上公开提供。

其实LLMs核心是一个自回归的 Transformer 模型。此模型根据输入 （prompt） 和到目前为止生成的输出标记的先前序列生成单词 （标记），一次生成一个单词 （标记）。对于每个请求，将重复这个昂贵的过程，直到模型输出终止令牌。这种顺序生成过程使工作负载受内存限制，未充分利用 GPU 的计算能力并限制了服务吞吐量。

#### continus batching 不是static batching


LLM生成一个完整token序列，当遇到stop token或最大句子长度就停止.
LLM decoder推理是memory bound的，这意味着推理throughput很大程度取决于你喂进HBM（High Bandwidth Memory 的缩写，指的是一种高带宽存储器）显存多大的batch size，而不是GPU算力越高，吞吐越大。
HBM的消耗随着model size和句子seqlen而变化，13b参数的模型对于seq中每个token的state都要花1M空间，那么对于A100-40G, 13b参数占了26g，
还剩14g可以保存14k token的state，
如果我们设seqlen为512，那么bs最大为28，
如果seqlen=2048，那么bs最大为7；
这是一个上限数字，因为还没算中间tensor的memory占用；

所以量化即quantization在LLM里面很有用，可以加大单卡上的batchsize和seqlen，但是这要去修改模型的weights，也有不用修改weights的，比如flashattention，
以及下文要提到的continuous batching，它们都提升了memory IO effeciency。



#### pagedAttention

pagedAttention这是一种注意力算法，其灵感来自操作系统中虚拟内存和分页的经典思想。与传统的注意力算法不同，PagedAttention 允许在非连续的内存空间中存储连续的键和值。具体来说，PagedAttention 将每个序列的 KV 缓存划分为多个块，每个块包含固定数量的令牌的键和值。在注意力计算期间，PagedAttention 内核会有效地识别并获取这些块。因为块不需要在内存中是连续的，所以可以像在操作系统的虚拟内存中一样以更灵活的方式管理键和值：可以将块视为页面，将令牌视为字节，将序列视为进程。序列的连续逻辑块通过块表映射到非连续的物理块。物理区块在生成新代币时按需分配。
在 PagedAttention 中，内存浪费仅发生在序列的最后一个块中。在实践中，这会导致近乎最佳的内存使用，仅浪费不到 4%。事实证明，内存效率的提高非常有益： 它允许系统一起批处理更多序列，提高 GPU 利用率，从而显著提高吞吐量。

PagedAttention 具有另一个关键优势：高效的内存共享。例如，在并行采样中，从同一提示符生成多个输出序列。在这种情况下，可以在输出序列之间共享提示的计算和内存。
PagedAttention 自然会通过其 block table 实现内存共享。与进程共享物理页的方式类似，PagedAttention 中的不同序列可以通过将其逻辑块映射到同一个物理块来共享块。为了确保安全共享，PagedAttention 会跟踪物理块的引用计数，并实现 Copy-on-Write 机制。

从 NVIDIA A100 到 H100，FLOPS 增加了 2 倍以上，但 GPU 内存保持在最大 80GB。因此，我们认为内存将成为越来越重要的瓶颈。
KV 缓存大小会随着请求数量的增加而快速增长。例如，对于 13B 参数 OPT 模型:
    单个令牌的 KV 缓存需要 800 KB 的空间，计算为:
     2（键和值向量）× 5120（隐藏状态大小）× 40（层数）× 2（每个 FP16 的字节数）。
由于 OPT 可以生成多达 2048 个令牌的序列，因此存储一个请求的 KV 缓存所需的内存可能高达 1.6 GB。并发 GPU 的内存容量为数十 GB。
即使所有可用内存都分配给 KV 缓存，也只能容纳几十个请求。

由于当前深度学习框架中的大多数运算符都要求将张量存储在连续的内存中，因此以前的LLM服务系统也将一个请求的 KV 缓存存储为跨不同位置的连续张量。
由于 的输出长度LLM不可预测，它们会根据请求的最大可能序列长度为请求静态分配内存块，而不管请求的实际输入或最终输出长度如何。

使用 PagedAttention 并构建了一个LLM服务引擎 vLLM，以应对挑战,对大型语言模型服务进行高效的内存管理.
vLLM 采用集中式调度程序来协调分布式 GPU 工作程序的执行。KV 缓存管理器以分页方式有效地管理 KV 缓存，由 PagedAttention 启用。
具体来说，KV 缓存管理器通过集中式调度器发送的指令来管理 GPU worker 上的物理 KV 缓存内存。

与传统的注意力算法不同，PagedAttention 允许在非连续的内存空间中存储连续的键和值。
PagedAttention 将每个序列的 KV 缓存划分为 KV 块。每个区块都包含固定数量的 Token 的 key 和 value 向量，我们将其表示为 KV

vLLM 为需要多个序列修改的物理块在块粒度上实施 copy-onwrite 机制，类似于操作系统虚拟内存中的写入时复制技术（例如，在分叉进程时）。

许多 LLMs GPU 的参数大小超过了单个 GPU 的容量。因此，有必要将它们划分到分布式 GPU 上，并以模型并行方式执行它们。
这需要能够处理分布式内存的内存管理器。vLLM 通过在 Transformer 上支持广泛使用的 Megatron-LM 风格张量模型并行策略 ，
在分布式环境中非常有效。此策略遵循 SPMD （Single Program Multiple Data） 执行计划，其中线性层被分区. 
执行计划来执行逐块矩阵乘法，GPU 通过 allreduce 操作不断同步中间结果。
具体来说，注意力操作符在注意力头维度上是分开的，每个 SPMD 过程都负责多头注意力中的注意力头子集。

#### 高性能CUDA kernel

由于 PagedAttention 引入了现有系统无法有效支持的内存访问模式，因此我们开发了几个 GPU 内核来优化它。
融合 reshape 和 block write。在每个 Transformer 层中，新的 KV 缓存被拆分为块，重塑为针对块读取优化的内存布局，然后保存在块表指定的位置。
为了最大限度地减少内核启动开销，我们将它们融合到单个内核中。
融合块读取和注意。在 FasterTransformer中适配了注意力内核，根据块表读取 KV 缓存，并动态进行注意力操作。
为了确保合并的内存访问，我们分配了一个 GPU warp 来读取每个块。此外，我们还在请求批处理中添加了对可变序列长度的支持。
熔合块复制。由 copy-on-write 机制发出的块复制操作可以在不连续的块上运行。
如果使用 cudaMemcpyAsync API，这可能会导致大量调用小数据移动。为了减轻开销，实现了一个内核，该内核将不同块的复制操作批处理到单个内核启动中。

vLLM 使用三种关键方法实现各种解码算法：fork、append 和 free。
fork 方法从现有序列创建新序列。
append 方法将新令牌附加到序列中。
free 方法删除序列。

例如，在并行采样中，vLLM 使用 fork 方法从单个输入序列创建多个输出序列。
然后，它在每次迭代中使用 append 向这些序列添加新标记，并使用 free 删除满足停止条件的序列。
相同的策略也适用于 vLLM 的光束搜索和前缀共享。结合这些方法也可以支持未来的解码算法。

#### vLLM架构
vLLM 采用一种集中式调度器（scheduler）来协调分布式 GPU 工作器（worker）的执行。**KV 缓存管理器由 PagedAttention 驱动，能以分页方式有效管理 KV 缓存**。具体来说，KV 缓存管理器通过集中式调度器发送的指令来管理 GPU 工作器上的物理 KV 缓存内存。

![](./vLLM/image_T52eX-wNY8.png)

#### 提供离线和在线推理服务模式

vLLM 可用于离线推理和在线服务。要使用 vLLM 进行离线推理，可以导入 vLLM 并在 Python 脚本中使用 LLM 类：
```Python
from vllm import LLM

prompts = ["Hello, my name is", "The capital of France is"]  # Sample prompts.
llm = LLM(model="lmsys/vicuna-7b-v1.3")  # Create an LLM.
outputs = llm.generate(prompts)  # Generate texts from the prompts.
```

要使用 vLLM 进行在线服务，可以通过以下方式启动与 OpenAI API 兼容的服务器：

```Python
python -m vllm.entrypoints.openai.api_server --model lmsys/vicuna-7b-v1.3
```

可以使用与 OpenAI API 相同的格式查询服务器：
```python
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "lmsys/vicuna-7b-v1.3",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'
```

